{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOE2cO1WZlm2b9LkxU1CNiw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Return prediction by LSTM\n","This is a part of the implementation conducted in my university coursework (DSS: Data Science School). Since the real data is confidential, this file is read-only."],"metadata":{"id":"mYGuZKueUW7V"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTS0pU2HUUlk"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 200)\n","pd.set_option('display.max_columns', 130)\n","from datetime import datetime\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Dropout\n","import matplotlib.pyplot as plt\n","plt.style.use(\"default\")\n","%matplotlib inline"]},{"cell_type":"code","source":["# read files\n","df_train_VIF_norm = pd.read_csv('path/to/VIF_norm_vol_merged_df_cleaned.csv') # normalized then VIF 57 columns\n","df_train_all = pd.read_csv('path/to/vol_merged_df_cleaned.csv')               # not normalized yet 124 columns\n","df_test = pd.read_csv('path/to/filtered_vol_merged_df_test.csv')              # not normalized yet 124 columns"],"metadata":{"id":"c-jDCTNqUVv7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_VIF_norm"],"metadata":{"id":"DnOSDiicWLyc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_VIF_norm.columns"],"metadata":{"id":"EWrXI8fEXm6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_all"],"metadata":{"id":"ZSuZB7glXCY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test"],"metadata":{"id":"iMss7km7XlI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# concatenate train and test\n","\n","# explanatory variables\n","variables = df_train_VIF_norm.columns\n","\n","# filter by explanatory variables\n","df_train_filtered = df_train_all.filter(variables)\n","df_train_filtered = df_train_filtered.drop(['flag_up', 'flag_down', 'Dates'], axis=1)\n","df_train_filtered.index = pd.to_datetime(df_train_all['Dates'])\n","\n","df_test_filtered = df_test.filter(variables)\n","df_test_filtered = df_test_filtered.drop(['flag_up', 'flag_down', 'Dates'], axis=1)\n","df_test_filtered.index = pd.to_datetime(df_test['Dates'])\n","\n","# concat\n","df_all_data = pd.concat([df_train_filtered, df_test_filtered], axis=0)"],"metadata":{"id":"JDMTzgR5XrEr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_all_data"],"metadata":{"id":"HiqtxzsyYt5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# normalize all data\n","\n","# scale objective variable\n","scaler1 = MinMaxScaler(feature_range=(0,1))\n","scaled_target = scaler1.fit_transform(df_all_data[['ESIndex']])\n","\n","# scale explanatory variables\n","scaler2 = MinMaxScaler(feature_range=(0,1))\n","scaled_variables = scaler2.fit_transform(df_all_data.drop(['ESIndex'], axis=1))\n","\n","# concat scaled data\n","scaled_data = np.concatenate([scaled_target, scaled_variables], axis=1)\n","print(scaled_data)\n","print(scaled_data.shape)"],"metadata":{"id":"XJt1-1w-YvQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# length of train data\n","training_data_len = len(df_train_filtered)\n","print(training_data_len)"],"metadata":{"id":"ArZAki2Ma47e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# period for prediction\n","window_size = 200\n","\n","train_data = scaled_data[:int(training_data_len),:]\n","\n","# generate x_train & y_train from train_data\n","x_train, y_train = [], []\n","for i in range(window_size, len(train_data)-20+1):\n","    x_train.append(train_data[i-window_size:i, :]) # window_size * number of variables\n","    y_train.append(train_data[i:i+20, 0])          # 20 days of return\n","\n","# change to numpy array\n","x_train, y_train = np.array(x_train), np.array(y_train)\n","x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n","\n","print(f'x_train.shape: {x_train.shape}')\n","print(f'y_train.shape: {y_train.shape}')"],"metadata":{"id":"Tdw2EOLobBPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM\n","model = Sequential()\n","model.add(LSTM(units=50,input_shape=(x_train.shape[1], x_train.shape[2]))) # input shape = (200, number of variables)\n","model.add(Dropout(0.2))\n","model.add(LSTM(units=50,return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(units=50,return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(units=50))\n","model.add(Dropout(0.2))\n","model.add(Dense(units=20)) # output shape = (20,)\n","\n","model.compile(optimizer='adam', loss='mean_squared_error')\n","x_train = np.asarray(x_train).astype('float32')\n","y_train = np.asarray(y_train).astype('float32')\n","history = model.fit(x_train, y_train, batch_size=32, epochs=20)"],"metadata":{"id":"8Lx5SQxdbqX2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"nU6qTMATcB9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preprocess test data\n","\n","'''\n","テストデータの各日付に対して次の20日間の値を予測するので\n","訓練データからは(window_size-1)日分もらう\n","'''\n","test_data = scaled_data[training_data_len - window_size + 1:, :]\n","\n","print(f'len(test_data): {len(test_data)}')\n","\n","x_test = []\n","y_test = scaled_data[training_data_len:, 0] # test GT\n","for i in range(window_size, len(test_data)+1):\n","    x_test.append(test_data[i-window_size:i, :])\n","\n","print(f'len(x_test): {len(x_test)}')\n","\n","# change to numpy array\n","x_test = np.array(x_test)\n","x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2])) # 変数の数に注意\n","\n","print(f'x_test.shape: {x_test.shape}')"],"metadata":{"id":"2jJjzvhMcEuK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predict\n","x_test = np.asarray(x_test).astype('float32')\n","scaled_predictions = model.predict(x_test)\n","\n","# denormalization\n","predictions = scaler1.inverse_transform(scaled_predictions)\n","y_test = scaler1.inverse_transform([y_test])"],"metadata":{"id":"CEAnvduUcTE2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# adopt and plot the first day of each 20-days prediction\n","\n","# prepare dataframe for plot\n","df_plot = pd.DataFrame()\n","df_plot.index = df_all_data.index[-1841:]\n","df_plot['Return_val'] = y_test[0]         # return validation\n","df_plot['Return_pred'] = predictions[:,0] # return prediction\n","df_plot.plot(figsize=(20,6))"],"metadata":{"id":"PN5ID5o4clSp"},"execution_count":null,"outputs":[]}]}